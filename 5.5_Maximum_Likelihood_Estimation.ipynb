{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Section 5.5 - Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation takes a distribution of values $\\mathbb{X} = \\{\\mathbf{x_{1}},...,\\mathbf{x_{i}}\\}$ drawn independently from a distribution $p_{data}(\\mathbf{x})$ and attempts to retrieve the most probable parameterization of a distribution $p_{model}(\\mathbf{x};\\mathbf{\\theta})$, ($\\theta$), to have generated $\\mathbb{X}$. If the distribution family used for $p_{model}(\\mathbf{x};\\mathbf{\\theta})$ is a close approximate of the distribution family of $p_{data}(\\mathbf{x})$, this most-probable parameterization should produce the best-possible estimate of $p_{data}(\\mathbf{x})$.\n",
    "\n",
    "Intuitively, the formula for this is to simply find the ($\\theta$) which maximizes the overall probability of the distribution.\n",
    "\n",
    "$$\\theta_{ML} = \\displaystyle \\argmax_{\\theta} p_{model}(\\mathbb{X}; \\theta) \n",
    "= \\displaystyle \\argmax_{\\theta}\\prod_{i = 1}^{m}p_{model}(x^{i}; \\theta)$$\n",
    "\n",
    "The product can be simplified to a summation through a few simple properties. Argmax is invariant to scale, as the largest value will be the same regardless of scaling factor. Thus we can freely apply the natural logarithm to the entire term $\\prod_{i = 1}^{m}p_{model}(x^{i}; \\theta) $. By taking advantage of the fact that $ln(a*b) = ln(a) + ln(b)$ we can turn this element into $\\sum_{i = 1}^{m}ln(p_{model}(x^{i}; \\theta))$\n",
    "\n",
    "Again invoking scaling invariance we can attach a term of $\\frac{1}{m}$ to complete the transformation of this term into the expected value of the natural logarithm of our probability function: \n",
    "\n",
    "$$\\theta_{ML} = \\displaystyle \\argmax_{\\theta} \\mathbb{E}_{\\mathbf{x}\\sim\\hat{p}_{data}}(ln(p_{model}(\\mathbf{x}; \\mathbf{\\theta}))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be extended to the case where our probability distribution is conditional, $P(\\mathbf{y} | \\mathbf{x};\\mathbf{\\theta})$. Assuming all examples are independent and identically distributed (i.i.d.), their sum probability is simply the product and we an apply the prior formula to achieve a similar equation:\n",
    "\n",
    "$$\\theta_{ML} = \\displaystyle \\argmax_{\\theta} \\sum_{i = 1}^{m}ln(P(\\mathbf{y^{i}}|\\mathbf{x^{i}}; \\mathbf{\\theta}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this understanding, we reframe our basic linear regression algorithm in these terms. Think of the model as producing a probability distribution $p(y | \\mathbf{x})$. Given a training set of input values $\\mathbb{X} = \\{\\mathbf{x_{1}},...,\\mathbf{x_{i}}\\}$ and their corresponding labels, we strive to parameterize a distribution $p(y | \\mathbf{x};\\mathbf{\\theta})$ such that we maximize the probability of our distribution producing the corresponding labels given $\\mathbb{X}$. In this instance we select a Normal distribution such that $ p(y | \\mathbf{x}) = \\mathcal{N}(y;\\hat{y}(\\mathbf{x};\\mathbf{w}),\\sigma^{2})$, where $\\hat{y}(\\mathbf{x};\\mathbf{w})$ predicts our mean parameter. The variance parameter $\\sigma^{2}$ is assume to be fixed for the purposes of this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i = 1}^{m}ln(P(\\mathbf{y^{i}}|\\mathbf{x^{i}}; \\mathbf{\\theta})) = \\sum_{i = 1}^{m}ln( {1 \\over \\sigma \\sqrt{2\\pi} } e^{-{1 \\over 2}({y_{i}-\\hat{y}(\\mathbf{y_{i}};\\mathbf{w_{i}}) \\over \\sigma})^{2}} ) = m*ln({1 \\over \\sigma} {1 \\over \\sqrt{2\\pi}}) + \\sum_{i = 1}^{m}-{{(y_{i}-\\hat{y}(\\mathbf{y_{i}};\\mathbf{w_{i}}))^{2} \\over 2\\sigma^{2} }} $$\n",
    "$$ = m*(ln(1) - ln(\\sigma)) + m*(ln(1) - ln((2\\pi)^{0.5})) - \\sum_{i = 1}^{m}{{(y_{i}-\\hat{y}(\\mathbf{y_{i}};\\mathbf{w_{i}}))^{2} \\over 2\\sigma^{2} }} = -m*ln(\\sigma) - 0.5m*ln(2\\pi) - \\sum_{i = 1}^{m}{{(y_{i}-\\hat{y}(\\mathbf{y_{i}};\\mathbf{w_{i}}))^{2} \\over 2\\sigma^{2} }} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the Mean-Squared-Error equation used for training linear regression: \n",
    "\n",
    "$$ MSE_{train} = {1 \\over m} \\sum_{i = 1}^{m}{||\\hat{y}_{i}-y_{i}||^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While similar, they are not the same equation. The important detail, however, is that both of these functions have the same optimum. The maximum likelihood estimation maximizes where $(y_{i}-\\hat{y}(\\mathbf{y_{i}};\\mathbf{w_{i}}))^{2}$ is as small as possible, and the MSE minimizes where $(\\hat{y}_{i}-y_{i})^{2}$ is as small as possible. This means that minimizing mean squared error is synonymous with maximizing the likelihood estimation, and justifies the use of MSE as a loss function for linear regression in terms of maximized likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
